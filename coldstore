#!/usr/bin/python
import ConfigParser
import argparse
from uuid import uuid1
from socket import gethostname
from time import strftime, time
from datetime import datetime
import boto
from boto import s3
import os.path
import sys
import json
import hashlib
import re
import tarfile
from pprint import pprint

from crypto import encryptFile, decryptFile

# globals connections
s3_conn = None
glacier_conn = None

#
# Parse config file
#
home_dir = os.path.expanduser('~')

config_file = "%s/.coldstore/coldstore.cfg" % (home_dir,)

if not os.path.isfile(config_file):
    sys.exit("Config file expected at ~/.coldstore/coldstore.cfg")

config = ConfigParser.RawConfigParser()
config.read(config_file)

ACCESS_KEY = config.get('Options', 'access_key')
SECRET_KEY = config.get('Options', 'secret_key')
TARGET_VAULT_NAME = config.get('Options', 'target_vault_name')
TARGET_BUCKET_NAME = config.get('Options', 'target_bucket_name')
enc_key = config.get('Options', 'encryption_key')

if len(enc_key) < 16:
    sys.exit("Encryption key must be at least 16 characters [recommend 32]")

#
# Parse command line options
#
arg_parser = argparse.ArgumentParser(prog="coldstore", description='Utility for archiving files to Amazon Glacier using S3 for metadata tracking')
arg_parser.add_argument("--config", help="Config file to load", metavar="file")
arg_parser.add_argument("--dry-run", help="Dry run, skip uploading", action="store_true")
arg_parser.add_argument("--skip-encryption", help="Skip encryption for single file", action="store_true")
arg_parser.add_argument("--replace", help="Replace archive/file if it exists", action="store_true")

arg_group = arg_parser.add_mutually_exclusive_group(required=True)
arg_group.add_argument(
    "-p", "--put", help="Put directory (as archive) or single file", metavar="/path")
arg_group.add_argument("-d", "--download", action="store_true", help="Path to get")
arg_group.add_argument(
    "-ls", "--list", help="list Vault structure", action="store_true")
arg_group.add_argument("-i", "--inspect",action="store_true", help="Show details for path")
arg_group.add_argument("--delete", action="store_true", help="Delete backup")

arg_parser.add_argument("dst", help="Destination path", metavar='[/path]', default="/")


args = arg_parser.parse_args()


def sha1File(fpath):
    sha = hashlib.sha1()
    with open(fpath, 'rb') as f:
        while True:
            chunk = f.read(2**10)
            if not chunk: break
            sha.update(chunk)
        return sha.hexdigest()

# use for generating meta data for single file upload
def generateFileMetadata(fpath):
    if not os.path.isfile(fpath):
        sys.exit("Can't generate metadata on file that doesn't exist: %s" % fpath)

    now_date = strftime("%B %d, %Y %H:%M:%S")
    fsize = os.path.getsize(fpath)
    host = gethostname()
    sha1 = sha1File(fpath)

    meta_data = {'upload_date': now_date,
                'timestamp': time(),
                'sha1': sha1,
                'size': fsize,
                'host': host,
                'metadata_version': "v1"
                }

    return meta_data

def inventoryDirectory(path):
    if not os.path.isdir(path):
        sys.exit("Can't inventory directory that doesn't exist: %s" % path)

    file_list = []
    for root, dirs, files in os.walk(path):
        for fname in files:
            stat = {}
            fpath = "%s/%s" % (root,fname)
            stat['name'] = fpath
            stat['size'] = os.path.getsize(fpath)
            stat['mtime'] = os.path.getmtime(fpath)
            file_list.append(stat)

    return file_list

def create_tgz(out, src_path):
    with tarfile.open(out, "w:gz") as tar:
        tar.add(src_path, arcname=os.path.basename(src_path))


#
# S3 logic
#

def getS3Conn():
    global s3_conn
    if not s3_conn:
        s3_conn = boto.connect_s3(
            aws_access_key_id = ACCESS_KEY,
            aws_secret_access_key = SECRET_KEY)

    return s3_conn

def checkKeyExists(key, bucket_name=TARGET_BUCKET_NAME):
    from boto.s3.key import Key

    conn = getS3Conn()
    bucket = conn.get_bucket(bucket_name)

    k = Key(bucket)

    k.key = key
    if k.exists():
        return True
    else:
        return False

def putArchiveMetadata(key, meta_data, bucket_name=TARGET_BUCKET_NAME):
    if args.dry_run:
        print "Dry run: Upload skipped"
        print "%s:%s" % (bucket_name, key)
        print json.dumps(meta_data)
        return 0

    conn = getS3Conn()
    bucket = conn.get_bucket(bucket_name)

    k = s3.key.Key(bucket)
    k.key = key
    k.set_contents_from_string(json.dumps(meta_data))

    return 0

def getArchiveMetadata(key, bucket_name=TARGET_BUCKET_NAME):
    conn = getS3Conn()
    bucket = conn.get_bucket(bucket_name)

    k = s3.key.Key(bucket)
    k.key = key
    if k.exists():
        md_json = k.get_contents_as_string()
    else:
        return None

    return md_json

def listArchives(path=None, bucket_name=TARGET_BUCKET_NAME):
    conn = getS3Conn()
    bucket = conn.get_bucket(bucket_name)

    for key in bucket.list(prefix=path):
        print "-",key.key

    return 0

def deleteKey(path, bucket_name=TARGET_BUCKET_NAME):
    conn = getS3Conn()
    bucket = conn.get_bucket(bucket_name)

    bucket.delete_key(path)

    return 0

#
# Glacier connection logic
#
def connect_glacier():
    glacier_connection = boto.connect_glacier(aws_access_key_id=ACCESS_KEY, aws_secret_access_key=secret_key)

    return glacier_connection

def getVault(glacier_connection):
    return glacier_connection.get_vault(TARGET_VAULT_NAME)


def uploadFileGlacier(fpath):
    if not os.path.isfile(fpath):
        sys.exit("Can't upload a file that doesn't exist: %s" % file_name)

    if args.dry_run:
        archive_id = str(uuid1())
    else:
        #archive_id = vault.upload_archive(archive)
        archive_id = str(uuid1())

    return archive_id

def getFile():

    return 0

def deleteFileGlacier(glacier_id, vault=TARGET_VAULT_NAME,):
    if args.dry_run:
        print "Dry run: Glacier delete skipped"
        print "%s:%s" % (vault, glacier_id)
        return 0

    print "Deleting %s from %s" % (glacier_id, vault)
    return 0


def main():
    #
    # Put file or directory
    #
    if args.put:
        src = args.put
        dst = args.dst

        if not re.match("^\/.*", dst):
            sys.exit("Destination must be a path")

        src_path = os.path.abspath(src)
        #
        # single file
        #
        if os.path.isfile(src_path):
            blah ,fname = os.path.split(src_path)
            print "Uploading file %s" % src

            if re.match('.*\/$', dst):
                file_dst = "%s%s" % (dst, fname)
            else:
                #file_dst = "%s/%s" % (dst, fname)
                file_dst = dst

            print "Destination path: %s" % file_dst

            # check for existing metadata indicating path exists
            if checkKeyExists(file_dst):
                if not args.replace:
                    sys.exit("Key already exists, to replace rerun with --replace")
                else:
                    md_json = getArchiveMetadata(file_dst)
                    metadata = json.loads(md_json)
                    glacier_id = metadata['glacier_id']
                    glacier_vault = metadata['glacier_vault']
                    deleteFileGlacier(glacier_id, glacier_vault)

            meta_data = generateFileMetadata(src_path)
            meta_data['type'] = "file"
            meta_data['source_path'] = src_path

            if args.skip_encryption:
                print "WARNING: Skipping encryption, file should already be encrypted"
                #actually catch the prompt later
                meta_data['skipped_encryption'] = "true"
                glacier_id = uploadFileGlacier(src_path)
            else:
                meta_data['skipped_encryption'] = "false"
                tmp_encrypt = "/tmp/coldstore-%s.enc" % uuid1()
                encryptFile(src_path, tmp_encrypt, enc_key)
                glacier_id = uploadFileGlacier(tmp_encrypt)
                os.remove(tmp_encrypt)

            if not glacier_id:
                sys.exit("Glacier upload failed!")

            meta_data['glacier_id'] = glacier_id
            meta_data['glacier_vault'] = TARGET_VAULT_NAME

            #print json.dumps(meta_data)
            putArchiveMetadata (file_dst, meta_data)

        #
        # Directory - creates tgz of folder and encrypts it before unlpoading
        #
        elif os.path.isdir(src_path):
            print "O, you're gonna make this hard with a directory"

            # if dst ends in a / then append source path into folder,
            # else just use destination as
            if re.match(".*\/$", dst) and not re.match(".*\/$", src):
                folder = os.path.basename(src)
                dst = "%s%s" % (dst, folder)

            # since its a folder, it has to end in / to indicate that
            if not re.match(".*\/$", dst):
                dst = "%s/" % dst


            #expand to full path
            src_path = os.path.abspath(src)
            print "Uploading to %s" % dst

            # check for existing metadata indicating path exists
            if checkKeyExists(dst):
                if not args.replace:
                    sys.exit("Key already exists, to replace rerun with --replace")
                else:
                    md_json = getArchiveMetadata(file_dst)
                    metadata = json.loads(md_json)
                    glacier_id = metadata['glacier_id']
                    glacier_vault = metadata['glacier_vault']
                    deleteFileGlacier(glacier_id, glacier_vault)

            inventory = inventoryDirectory(src_path)

            tmp_tgz = "/tmp/coldstore-%s.tgz" % uuid1()
            create_tgz(tmp_tgz, src_path)

            meta_data = generateFileMetadata(tmp_tgz)
            meta_data['type'] = "folder"
            meta_data['source_path'] = src_path
            meta_data['inventory'] = inventory

            tmp_encrypt = "%s.enc" % (tmp_tgz)
            encryptFile(tmp_tgz, tmp_encrypt, enc_key)

            glacier_id = uploadFileGlacier(tmp_encrypt)

            meta_data['glacier_id'] = glacier_id
            meta_data['glacier_vault'] = TARGET_VAULT_NAME

            os.remove(tmp_tgz)
            os.remove(tmp_encrypt)

            if not glacier_id:
                sys.exit("Glacier upload failed!")

            #print json.dumps(meta_data)
            putArchiveMetadata(dst, meta_data)

        # path for put doesn't exist
        else:
            sys.exit("Cannot locate source %s" % (src))

        print "Success"

    elif args.download:
        print "Let's go get some data"
        #getFile(args.get, vault)

    elif args.list:
        print "Showing known archives"
        dst = args.dst
        if re.match('^/.*', dst):
            dst = dst[1:]

        listArchives(dst)

    elif args.inspect:
        print "Inspect"
        key = args.dst
        if key == "/":
            sys.exit("Can't inspect /")

        #print key

        md_json = getArchiveMetadata(key)

        if md_json:
            pprint(json.loads(md_json))
        else:
            sys.exit("Key does not exists, sorry")

    elif args.delete:
        key = args.dst
        print "Delete %s" % key
        if key == "/":
            sys.exit("Can't delete /")

        if re.match('^/.*', key):
            key = key[1:]

        md_json = getArchiveMetadata(key)
        if md_json:
            metadata = json.loads(md_json)
            glacier_id = metadata['glacier_id']
            glacier_vault = metadata['glacier_vault']
            deleteFileGlacier(glacier_id, glacier_vault)
            deleteKey(key)
        else:
            sys.exit("Unkown key")


    else:
        sys.exit("Not sure what I was supposed to do here ...")

if __name__ == "__main__":
  main()
